{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyOfIbk5jjYe5twCs6w2tWAr",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Gugo-le/103-zoom/blob/main/NLP.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 토큰화"
      ],
      "metadata": {
        "id": "lNQGqSS0n6P4"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RRF9g2n7nv-D",
        "outputId": "a82b2f1f-d732-4f17-a8b8-4590859be8e8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[1, 2, 3, 4, 5], [1, 2, 3, 6, 5], [2, 7, 4, 1]]\n",
            "{'today': 1, 'is': 2, 'a': 3, 'sunny': 4, 'day': 5, 'rainy': 6, 'it': 7}\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "\n",
        "sentences = [\n",
        "    'Today is a sunny day',\n",
        "    'Today is a rainy day',\n",
        "    'Is it sunny today?'\n",
        "]\n",
        "\n",
        "tokenizer = Tokenizer(num_words=100) # Tokenizer 객체를 만들 때 토큰화 할 수 있는 단어 개수를 지정합니다.(이 값은 말뭉치에서 추출할 수 있는 최대 토큰 개수입니다.)\n",
        "tokenizer.fit_on_texts(sentences) # fit_on_texts를 호출해 토큰화된 단어 인덱스를 만듭니다.\n",
        "word_index = tokenizer.word_index # word_index: 말뭉치 안에 있는 단어와 인덱스의 키/값 쌍이 출력됨.\n",
        "\n",
        "print(word_index)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 문장을 시퀀스로 바꾸기"
      ],
      "metadata": {
        "id": "4zq5FwKXuI2L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "\n",
        "sentences = [\n",
        "    'Today is a sunny day',\n",
        "    'Today is a rainy day',\n",
        "    'Is it sunny today?'\n",
        "]\n",
        "\n",
        "tokenizer = Tokenizer(num_words=100) # Tokenizer 객체를 만들 때 토큰화 할 수 있는 단어 개수를 지정합니다.(이 값은 말뭉치에서 추출할 수 있는 최대 토큰 개수입니다.)\n",
        "tokenizer.fit_on_texts(sentences) # fit_on_texts를 호출해 토큰화된 단어 인덱스를 만듭니다.\n",
        "word_index = tokenizer.word_index # word_index: 말뭉치 안에 있는 단어와 인덱스의 키/값 쌍이 출력됨.\n",
        "\n",
        "sequences = tokenizer.texts_to_sequences(sentences)\n",
        "print(sequences)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rt6LEkfZn_1g",
        "outputId": "21e670e3-bd6c-4652-ec0d-3eefcfc1f7c6"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[1, 2, 3, 4, 5], [1, 2, 3, 6, 5], [2, 7, 4, 1]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# OOV 토큰 사용하기"
      ],
      "metadata": {
        "id": "LQCOqyf_u6NO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_data = [\n",
        "    'Today is a snowy day',\n",
        "    'Will it be rainy tomorrow?'\n",
        "]\n",
        "\n",
        "test_seq = tokenizer.texts_to_sequences(test_data)\n",
        "print(word_index)\n",
        "print(test_seq)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XHR9Ih_Nu260",
        "outputId": "21433fb7-774f-4ffe-851f-2d10572ae48c"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'today': 1, 'is': 2, 'a': 3, 'sunny': 4, 'day': 5, 'rainy': 6, 'it': 7}\n",
            "[[1, 2, 3, 5], [7, 6]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = Tokenizer(num_words=100, oov_token = \"<OOV>\") # oov토큰은 Tokenizer 객체를 만들 때 지정할 수 있으며 oov_token 매개변수로 설정합니다.(* 어떤 문자로도 지정할 수 있지만 현재 말뭉치에 등장하지 않는 문자열이어야 합니다.)\n",
        "tokenizer.fit_on_texts(sentences) # fit_on_texts를 호출해 토큰화된 단어 인덱스를 만듭니다.\n",
        "word_index = tokenizer.word_index # word_index: 말뭉치 안에 있는 단어와 인덱스의 키/값 쌍이 출력됨.\n",
        "\n",
        "test_seq = tokenizer.texts_to_sequences(test_data)\n",
        "print(word_index)\n",
        "print(test_seq)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_4qOUW1Nu29V",
        "outputId": "e42aff8d-2050-474d-8c17-78e7197b2534"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'<OOV>': 1, 'today': 2, 'is': 3, 'a': 4, 'sunny': 5, 'day': 6, 'rainy': 7, 'it': 8}\n",
            "[[2, 3, 4, 1, 6], [1, 8, 1, 7, 1]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# TeztVectorization 층 사용하기"
      ],
      "metadata": {
        "id": "26h8nyD1xzjK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tv = keras.layers.TextVectorization(max_tokens=100)\n",
        "tv.adapt(sentences)\n"
      ],
      "metadata": {
        "id": "h-Jxem26xzAC"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tv.get_vocabulary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E_1XiG1jxzCl",
        "outputId": "fc739c74-07c4-4ffd-d1f6-5eb621521bd9"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['', '[UNK]', 'today', 'is', 'sunny', 'day', 'a', 'rainy', 'it']"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_seq = tv(test_data)\n",
        "test_seq.numpy()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RrSLioYVxzE5",
        "outputId": "386de953-d76a-4e23-df96-2c7579a8d3a1"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[2, 3, 6, 1, 5],\n",
              "       [1, 8, 1, 7, 1]])"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 패딩 이해하기"
      ],
      "metadata": {
        "id": "O5AXXdpCygLQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sentences = [\n",
        "    'Today is a sunny day',\n",
        "    'Today is a rainy day',\n",
        "    'Is it sunny today?',\n",
        "    'I really enjoyed walking in the snow today'\n",
        "]\n",
        "tokenizer.fit_on_texts(sentences)\n",
        "sequences = tokenizer.texts_to_sequences(sentences)\n",
        "print(sequences)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5A7FiAfLyi2V",
        "outputId": "ac578c9d-c44e-4fbc-8286-d02e46510994"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[2, 3, 4, 5, 6], [2, 3, 4, 7, 6], [3, 8, 5, 2], [9, 10, 11, 12, 13, 14, 15, 2]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "padded = pad_sequences(sequences, padding = 'post', maxlen = 6, truncating='post')\n",
        "print(padded)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lrz5Vfnsyi46",
        "outputId": "c1f084b2-d6cc-4209-ab28-099cf5e572bf"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[ 2  3  4  5  6  0]\n",
            " [ 2  3  4  7  6  0]\n",
            " [ 3  8  5  2  0  0]\n",
            " [ 9 10 11 12 13 14]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "6S2XCwvtyi7e"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}